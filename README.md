# Ideas to write better prompts

## What the LLM can do:


*If you want to learn how to write effective prompts that inspire thoughtful responses and engage your audience, you've come to the right place. In this repository, we'll explore the key elements of successful prompts and provide tips and examples to help you create prompts that are clear, concise, and motivating. Whether you're a teacher, writer, or content creator, the insights and strategies shared here can help you craft prompts that elicit the best possible outcomes from your audience.*


This was a text generated by ChatGPT. Really cool! Besides ChatGPT, which is the most prominent LLM, there are others like OpenAssistant and the new Bing chat. This repository is for my personal use, so I'm not explaining big stuff here, others have done that much better. The goal is simply to write better prompts. Maybe there is something here.

## English vs. Other Languages

ChatGPT as the most prominent example can speak over 95 languages, no idea how many there are at this point, but chances are very high that ChatGPT also speaks your language, even if it is a programming language.
https://twitter.com/DataChaz/status/1629766184328364034

But as it turns out, english is by far the most powerful language. You can see that in benchmarks that I have to look up again here. Or you can see in Bing Chat, for example, how my prompt written in German is translated into English when you search. The reason is simply a much higher accuracy in english. 

**English can be a big advantage, even if you prefer to prompt in German like I do.**

## Negation

I wasted hours telling ChatGPT what I don't want. This article addresses the problem somewhat. 
https://www.quantamagazine.org/ai-like-chatgpt-are-no-good-at-not-20230512/
To better understand how this issue affects my results, I need to do some more research on my own, but some things should be more true.

**Negation seems to work against the probability for better prompts.**

## Dont waste time!

Sometimes you might not want to take an LLM for your work. Interpolation and extrapolation are strange terms, where interpolation means what we have now, and extrapolation is the way that somehow goes beyond our frame.
An LLM was trained with stuff, by that I mean a lot of text, text that was not read. In my mind, such an LLM forms an average of the trained examples. So what has been trained often it can do particularly well, but at the borders it can quickly come to some problems, and beyond the borders confused outputs are rather the rule.

LLM's not only struggle with lack of training examples, they also have to keep up with our world. ChatGPT has been trained a few times so far. However, new methods will only be learned with the next update, and will then be on the outer edge of my mental map. Here I seem to have been more often, because the results were confused and have cost me time!

**Sometimes better you do!**
